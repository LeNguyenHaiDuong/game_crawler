import pandas as pd
import requests
from bs4 import BeautifulSoup
import time
import random
import sys
import os

batch_id = int(sys.argv[1])  # Nh·∫≠n batch_id t·ª´ GitHub Actions
batch_size = int(sys.argv[2])  # M·ªói workflow x·ª≠ l√Ω 10.000 d√≤ng

# X√°c ƒë·ªãnh ph·∫°m vi d√≤ng c·∫ßn x·ª≠ l√Ω
start_idx = batch_id * batch_size
end_idx = start_idx + batch_size

# ƒê·ªçc v√† ghi t·ª´ file n√†y
# ƒê·ªãnh nghƒ©a th∆∞ m·ª•c l∆∞u file (game_crawler/data/)
BASE_DIR = os.path.abspath(os.path.join(os.path.dirname(__file__), ".."))  # Tr·ªè ƒë·∫øn th∆∞ m·ª•c game_crawler/
DATA_DIR = os.path.join(BASE_DIR, "data")  
output_file = os.path.join(DATA_DIR, f"vgsales_updated_{batch_id}.csv")

USER_AGENTS = [
    # üåê Chrome (Windows)
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36",
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/118.0.5993.88 Safari/537.36",
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/117.0.5938.132 Safari/537.36",

    # üçè Chrome (MacOS)
    "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
    "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36",

    # üêß Chrome (Linux)
    "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/118.0.0.0 Safari/537.36",
    "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/117.0.0.0 Safari/537.36",

    # üî• Firefox (Windows)
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:120.0) Gecko/20100101 Firefox/120.0",
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:119.0) Gecko/20100101 Firefox/119.0",
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:118.0) Gecko/20100101 Firefox/118.0",

    # üçè Firefox (MacOS)
    "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7; rv:120.0) Gecko/20100101 Firefox/120.0",
    "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_6; rv:119.0) Gecko/20100101 Firefox/119.0",

    # üè¢ Edge (Windows)
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36 Edg/120.0.0.0",
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36 Edg/119.0.0.0",

    # üì± Mobile (iPhone)
    "Mozilla/5.0 (iPhone; CPU iPhone OS 16_0 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/16.0 Mobile/15E148 Safari/604.1",
    "Mozilla/5.0 (iPhone; CPU iPhone OS 15_6 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/15.6 Mobile/15E148 Safari/604.1",

    # üì± Mobile (Android)
    "Mozilla/5.0 (Linux; Android 13; SM-S908B) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Mobile Safari/537.36",
    "Mozilla/5.0 (Linux; Android 12; Pixel 6 Pro) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Mobile Safari/537.36",

    # ü§ñ Googlebot (Crawler)
    "Mozilla/5.0 (compatible; Googlebot/2.1; +http://www.google.com/bot.html)",
    "Mozilla/5.0 (Linux; Android 10; SM-G975F) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Mobile Safari/537.36 (compatible; Googlebot/2.1; +http://www.google.com/bot.html)",

    # üïµÔ∏è Bingbot (Crawler)
    "Mozilla/5.0 (compatible; bingbot/2.0; +http://www.bing.com/bingbot.htm)",
]


if os.path.exists(output_file):
    # ƒê·ªçc file CSV g·ªëc
    print("ƒêang t·∫£i d·ªØ li·ªáu t·ª´ vgsales_updated.csv ƒë·ªÉ ti·∫øp t·ª•c c·∫≠p nh·∫≠t.")
    cols_to_read = ["Rank", "Genre"]  # Ch·ªâ ƒë·ªçc c·ªôt c·∫ßn thi·∫øt
    df = pd.read_csv(output_file, usecols=cols_to_read, low_memory=False)
    print(f"‚úÖ Processing Batch {batch_id}: Rows {start_idx} to {end_idx}")
else:
    df = pd.read_csv("data/vgsales.csv", low_memory=False)
    df = df.iloc[start_idx:end_idx]
    df["Developers"] = "Unknown"  # Th√™m c·ªôt m·ªõi v√†o DataFrame
    print(f"Kh√¥ng t√¨m th·∫•y {output_file}, s·∫Ω t·∫°o file m·ªõi.")

# Ki·ªÉm tra c·ªôt Genre c√≥ t·ªìn t·∫°i
if "Genre" not in df.columns:
    raise ValueError("C·ªôt 'Genre' kh√¥ng t·ªìn t·∫°i trong file CSV!")

def get_Info(url):
    headers = {
        "User-Agent": random.choice(USER_AGENTS)
    }
    try:
        response = requests.get(url, headers=headers, timeout=10)
        response.raise_for_status()  # Ki·ªÉm tra l·ªói HTTP
        soup = BeautifulSoup(response.text, "html.parser")
        
        # T√¨m div ch·ª©a th√¥ng tin game
        info_box = soup.find("div", {"id": "gameGenInfoBox"})
        if not info_box:
            return "Unknown"  # N·∫øu kh√¥ng t√¨m th·∫•y th√¥ng tin

        h2s = info_box.find_all("h2")
        for h2 in h2s:
            if h2.string == "Genre":
                genre = h2.find_next_sibling("p").text.strip()

            elif h2.string == "Developer":
                devs = h2.find_next_sibling("p")  # L·∫•y ph·∫ßn t·ª≠ ƒë·∫ßu ti√™n ch·ª©a developer
                if devs:
                    developers = ", ".join([p.text.strip() for p in info_box.find_all("p") if p.find_previous_sibling("h2") and p.find_previous_sibling("h2").text == "Developer"])

        return genre, developers  # Tr·∫£ v·ªÅ c·∫£ Genre v√† Developers
    
    except:
        print("Cannot get:", url)
        return url, "Unknown"  # Tr·∫£ v·ªÅ l·ªói ƒë·ªÉ x·ª≠ l√Ω sau

def process_row(idx, url):
    """H√†m x·ª≠ l√Ω t·ª´ng d√≤ng, g·ªçi get_Info v·ªõi URL."""
    genre, developers = get_Info(url)
    return idx, genre, developers

# L·ªçc c√°c d√≤ng c·∫ßn c·∫≠p nh·∫≠t (ch·ªâ c·∫≠p nh·∫≠t n·∫øu v·∫´n l√† URL)
rows_to_update = [(idx, row["Genre"]) for idx, row in df.iterrows() if len(row["Genre"]) > 20]
if len(rows_to_update) == 0:
    print("This batch is already done.")
else:
    print(f"Done filtering, update list from {rows_to_update[0]}")


batch_size_update = 100
count = 0

for idx, url in rows_to_update:
    idx, new_Genre, new_Developers = process_row(idx, url)  # G·ªçi h√†m l·∫•y Genre & Developers
    while len(new_Genre) > 20:
        df.to_csv(output_file, index=False)
        print(f"Saved until line {idx}")
        print("Retry in 180s")
        time.sleep(180)
        idx, new_Genre, new_Developers = process_row(idx, url)  # G·ªçi l·∫°i h√†m l·∫•y d·ªØ li·ªáu

    print(f"Update indx {idx}")
    df.at[idx, "Genre"] = new_Genre  # C·∫≠p nh·∫≠t gi√° tr·ªã m·ªõi
    df.at[idx, "Developers"] = new_Developers  # C·∫≠p nh·∫≠t danh s√°ch nh√† ph√°t tri·ªÉn
    
    count += 1

    if count == batch_size_update:
        count = 0
        df.to_csv(output_file, index=False)
        print(f"Saved until line {idx}")

df.to_csv(output_file, index=False)
print(f"‚úÖ Saved {output_file}")


